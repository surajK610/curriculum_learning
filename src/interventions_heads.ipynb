{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0c75654-a00d-42d5-af7d-55805cb3721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "from typing import cast, Dict, List, Tuple, Union\n",
    "from typing_extensions import get_args, Literal\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import is_square\n",
    "from transformer_lens.head_detector import (compute_head_attention_similarity_score, \n",
    "                      get_previous_token_head_detection_pattern, \n",
    "                      get_duplicate_token_head_detection_pattern,\n",
    "                      get_induction_head_detection_pattern)\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('/users/sanand14/data/sanand14/learning_dynamics/src/experiments/utils')\n",
    "sys.path.append('/users/sanand14/data/sanand14/learning_dynamics/src/experiments')\n",
    "\n",
    "from aheads import create_repeats_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd9d88d-c619-4277-94d8-e306f1a05080",
   "metadata": {},
   "source": [
    "## TOY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7d2a3ba7-174f-4ae6-a706-161b4406297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(pl.LightningModule):\n",
    "    def __init__(self, num_features: int, num_interm : int):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(num_features, num_interm),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_interm, 1, bias=False)\n",
    "            )\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-2)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", logs[\"acc\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"train_loss\": logs[\"acc\"]}\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch)\n",
    "        self.log(\"val_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", logs[\"acc\"], on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"val_acc\": logs[\"acc\"]}\n",
    "    \n",
    "    def step(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.binary_cross_entropy(torch.sigmoid(logits), y)\n",
    "        acc = ((logits.squeeze() > 0.5).float() == y.squeeze()).float().mean()\n",
    "        return loss, {\"loss\": loss.item(), \"acc\": acc.item()}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x, _ = x\n",
    "        return self.body(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4dafcdc3-a1d9-453f-a4fa-6602fd5fd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generation hyperparameters\n",
    "\n",
    "size = 10000\n",
    "m1, m2, m3, m4 = 0, 1, 2, 3\n",
    "s1, s2, s3, s4 = 1, 1, 1, 1\n",
    "N, N_p, N_pp = 0, 2, 3\n",
    "vec_size = 4\n",
    "\n",
    "epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "595b5ff2-4935-40f5-93b5-e552051a28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_inputs(m1, s1, m2, s2, m3, s3, m4, s4, vec_size, N, N_p, N_pp, init_task=True):\n",
    "    f1 = torch.normal(mean=torch.tensor(m1).repeat((1, vec_size)).float(), std=s1)\n",
    "    f2 = torch.normal(mean=torch.tensor(m2).repeat((1, vec_size)).float(), std=s2)\n",
    "    f3 = torch.normal(mean=torch.tensor(m3).repeat((1, vec_size)).float(), std=s3)\n",
    "    f4 = torch.normal(mean=torch.tensor(m4).repeat((1, vec_size)).float(), std=s4)\n",
    "    if init_task:\n",
    "        full = f1\n",
    "        label = (torch.mean(f1) > N)\n",
    "        alt_label = (torch.mean(f1) > N)\n",
    "    else:\n",
    "        full = torch.concat([f1, f2, f3, f4], dim=1) \n",
    "        label = (torch.mean(f3) > N_p) if (torch.mean(f1) > N) else (torch.mean(f4) > N_pp)\n",
    "        alt_label = (torch.mean(f3) > N_p) if (torch.mean(f2) > N) else (torch.mean(f4) > N_pp)\n",
    "    return full, label, alt_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0d5f5d9-5ca3-4935-991d-3af02c013c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "alt_labels = []\n",
    "\n",
    "for i in (range(size)):\n",
    "    input, label, alt_label = generate_gaussian_inputs(m1, s1, m2, s2, m3, s3, m4, s4, vec_size, N, N_p, N_pp)\n",
    "    inputs.append(input)\n",
    "    labels.append(label)\n",
    "    alt_labels.append(alt_label)\n",
    "    \n",
    "inputs = torch.vstack(inputs)\n",
    "labels = torch.vstack(labels).float()\n",
    "alt_labels = torch.vstack(alt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e1845b8b-2db9-4ba9-871a-367cdc3cf928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | body | Sequential | 768   \n",
      "------------------------------------\n",
      "768       Trainable params\n",
      "0         Non-trainable params\n",
      "768       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▉  | 250/313 [00:00<00:00, 264.92it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]  \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80%|████████  | 251/313 [00:00<00:00, 263.72it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  81%|████████  | 252/313 [00:00<00:00, 263.92it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  81%|████████  | 253/313 [00:00<00:00, 264.16it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  81%|████████  | 254/313 [00:00<00:00, 264.38it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  81%|████████▏ | 255/313 [00:00<00:00, 264.61it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  82%|████████▏ | 256/313 [00:00<00:00, 264.83it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  82%|████████▏ | 257/313 [00:00<00:00, 265.02it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  82%|████████▏ | 258/313 [00:00<00:00, 265.22it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  83%|████████▎ | 259/313 [00:00<00:00, 265.44it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  83%|████████▎ | 260/313 [00:00<00:00, 265.65it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  83%|████████▎ | 261/313 [00:00<00:00, 265.87it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  84%|████████▎ | 262/313 [00:00<00:00, 266.09it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  84%|████████▍ | 263/313 [00:00<00:00, 266.32it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  84%|████████▍ | 264/313 [00:00<00:00, 266.52it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  85%|████████▍ | 265/313 [00:00<00:00, 266.47it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  85%|████████▍ | 266/313 [00:00<00:00, 266.37it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  85%|████████▌ | 267/313 [00:01<00:00, 266.21it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  86%|████████▌ | 268/313 [00:01<00:00, 266.20it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  86%|████████▌ | 269/313 [00:01<00:00, 266.19it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  86%|████████▋ | 270/313 [00:01<00:00, 266.15it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  87%|████████▋ | 271/313 [00:01<00:00, 265.92it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  87%|████████▋ | 272/313 [00:01<00:00, 265.83it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  87%|████████▋ | 273/313 [00:01<00:00, 265.53it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  88%|████████▊ | 274/313 [00:01<00:00, 265.65it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  88%|████████▊ | 275/313 [00:01<00:00, 265.48it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  88%|████████▊ | 276/313 [00:01<00:00, 265.33it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  88%|████████▊ | 277/313 [00:01<00:00, 265.11it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  89%|████████▉ | 278/313 [00:01<00:00, 263.73it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  89%|████████▉ | 279/313 [00:01<00:00, 263.66it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  89%|████████▉ | 280/313 [00:01<00:00, 263.44it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  90%|████████▉ | 281/313 [00:01<00:00, 263.42it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  90%|█████████ | 282/313 [00:01<00:00, 263.33it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  90%|█████████ | 283/313 [00:01<00:00, 263.34it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  91%|█████████ | 284/313 [00:01<00:00, 262.84it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  91%|█████████ | 285/313 [00:01<00:00, 262.58it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  91%|█████████▏| 286/313 [00:01<00:00, 262.39it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  92%|█████████▏| 287/313 [00:01<00:00, 262.34it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  92%|█████████▏| 288/313 [00:01<00:00, 262.02it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  92%|█████████▏| 289/313 [00:01<00:00, 262.06it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  93%|█████████▎| 290/313 [00:01<00:00, 261.97it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  93%|█████████▎| 291/313 [00:01<00:00, 261.86it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  93%|█████████▎| 292/313 [00:01<00:00, 260.32it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  94%|█████████▎| 293/313 [00:01<00:00, 258.62it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  94%|█████████▍| 294/313 [00:01<00:00, 258.53it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  94%|█████████▍| 295/313 [00:01<00:00, 258.52it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  95%|█████████▍| 296/313 [00:01<00:00, 258.39it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  95%|█████████▍| 297/313 [00:01<00:00, 258.34it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  95%|█████████▌| 298/313 [00:01<00:00, 258.21it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  96%|█████████▌| 299/313 [00:01<00:00, 258.13it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  96%|█████████▌| 300/313 [00:01<00:00, 258.11it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  96%|█████████▌| 301/313 [00:01<00:00, 257.98it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  96%|█████████▋| 302/313 [00:01<00:00, 256.48it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  97%|█████████▋| 303/313 [00:01<00:00, 256.49it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  97%|█████████▋| 304/313 [00:01<00:00, 256.13it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  97%|█████████▋| 305/313 [00:01<00:00, 256.15it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  98%|█████████▊| 306/313 [00:01<00:00, 256.10it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  98%|█████████▊| 307/313 [00:01<00:00, 256.14it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  98%|█████████▊| 308/313 [00:01<00:00, 255.99it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  99%|█████████▊| 309/313 [00:01<00:00, 255.92it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  99%|█████████▉| 310/313 [00:01<00:00, 255.80it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0:  99%|█████████▉| 311/313 [00:01<00:00, 255.84it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0: 100%|█████████▉| 312/313 [00:01<00:00, 255.64it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000]\n",
      "Epoch 0: 100%|██████████| 313/313 [00:01<00:00, 253.72it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000, val_loss_step=0.00144, val_acc_step=1.000, val_loss_epoch=0.0326, val_acc_epoch=0.985]\n",
      "Epoch 0: 100%|██████████| 313/313 [00:01<00:00, 253.32it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000, val_loss_step=0.00144, val_acc_step=1.000, val_loss_epoch=0.0326, val_acc_epoch=0.985, train_loss_epoch=0.0695, train_acc_epoch=0.969]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:01<00:00, 249.27it/s, loss=0.0347, v_num=13, train_loss_step=0.0111, train_acc_step=1.000, val_loss_step=0.00144, val_acc_step=1.000, val_loss_epoch=0.0326, val_acc_epoch=0.985, train_loss_epoch=0.0695, train_acc_epoch=0.969]\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(inputs))\n",
    "\n",
    "inputs_t, labels_t, alt_labels_t = inputs[:train_size], labels[:train_size], alt_labels[:train_size]\n",
    "inputs_v, labels_v, alt_labels_v = inputs[train_size:], labels[train_size:], alt_labels[train_size:]\n",
    "\n",
    "train_dataset = TensorDataset(inputs_t.detach(), labels_t.view(-1, 1))\n",
    "val_dataset = TensorDataset(inputs_v.detach(), labels_v.view(-1, 1))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "toy_model = ToyModel(inputs_t.shape[1], 128).to(device)\n",
    "trainer = Trainer(max_epochs=epochs)\n",
    "trainer.fit(toy_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c54aabaf-a669-4ab4-9573-96c43cfab40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 63/63 [00:00<00:00, 220.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9854999780654907     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03262592479586601    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9854999780654907    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03262592479586601   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 0.03262592479586601, 'val_acc_epoch': 0.9854999780654907}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(toy_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6030a342-3a72-466d-82f8-0f500ff07e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=128, out_features=1, bias=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_model.body[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f46951-c624-4a83-ac2a-03e48bf810be",
   "metadata": {},
   "source": [
    "## TRANSPLANTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cade8b7-74df-4c5e-ada9-469b4f5c8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHIA_VOCAB_SIZE = 50277 #50304\n",
    "N_LAYERS=12\n",
    "MODEL = \"EleutherAI/pythia-160m\"\n",
    "PYTHIA_CHECKPOINTS_OLD = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512] + list(range(1000, 143000 + 1, 10000)) + [143000]\n",
    "PYTHIA_CHECKPOINTS = [512] + list(range(1000, 10000 + 1, 1000))\n",
    "\n",
    "HeadName = Literal[\"previous_token_head\", \"duplicate_token_head\", \"induction_head\"]\n",
    "HEAD_NAMES = cast(List[HeadName], get_args(HeadName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a3f3d7-1203-4760-b3b8-594a801479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeats_dataset(num_samples=50, min_vector_size=5, max_vector_size=50, min_num_repeats=5, max_num_repeats=20, max_vocab=PYTHIA_VOCAB_SIZE):\n",
    "  \"\"\"Creates a dataset for the experiment.\"\"\"\n",
    "  dataset = []\n",
    "  for _ in range(num_samples):\n",
    "    vector_size = torch.randint(min_vector_size, max_vector_size, (1,)).item()\n",
    "    num_repeats = torch.randint(min_num_repeats, max_num_repeats, (1,)).item()\n",
    "    tokens = torch.randint(0, max_vocab, (1, vector_size))\n",
    "    tokens = tokens.repeat((1, num_repeats))\n",
    "    dataset.append(tokens)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed88b97-5755-4cb5-9252-bcd1f95c5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('../outputs/aheads/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa1061d-1a82-4180-975d-304e701a3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b8aaa62-84a9-4c7f-8611-a966b7c29a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attention_head(model1, model2, layer_idx, head_idx, dataset):\n",
    "  if model1.isinstance(HookedTransformer) and model2.isinstance(HookedTransformer):\n",
    "    model1.W_K.data[layer_idx, head_idx, :, :] = model2.W_K.data[layer_idx, head_idx, :, :]\n",
    "    model1.W_Q.data[layer_idx, head_idx, :, :] = model2.W_Q.data[layer_idx, head_idx, :, :]\n",
    "    model1.W_V.data[layer_idx, head_idx, :, :] = model2.W_V.data[layer_idx, head_idx, :, :]\n",
    "    model1.b_K.data[layer_idx, head_idx, :] = model2.b_K.data[layer_idx, head_idx, :]\n",
    "    model1.b_Q.data[layer_idx, head_idx, :] = model2.b_Q.data[layer_idx, head_idx, :]\n",
    "    model1.b_V.data[layer_idx, head_idx, :] = model2.b_V.data[layer_idx, head_idx, :]\n",
    "  else:\n",
    "    model1.encoder.layers[layer_idx].self_attn.in_proj_weight.data[head_idx,:,:] = model2.encoder.layers[layer_idx].self_attn.in_proj_weight.data[head_idx,:,:]\n",
    "  return perplexity(model1, dataset), perplexity(model2, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a891be09-fbcc-4487-aa72-42fdcfb25203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(corpus, model, device=\"cpu\"):\n",
    "    encoded_input = model.to_tokens(corpus)\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(encoded_input).squeeze(0)\n",
    "      loss = F.cross_entropy(outputs, encoded_input.squeeze(0), reduction='sum')/encoded_input.shape[1]\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe7c40a-eb0a-45a2-a8f6-86b2a581175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, dataset):\n",
    "  data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      inputs, targets = batch\n",
    "      outputs = model(inputs)\n",
    "      loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "      total_loss += loss.item()\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    return torch.exp(torch.tensor(average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b33722-f007-48c9-bbfd-0a231387a349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
