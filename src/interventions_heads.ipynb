{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c75654-a00d-42d5-af7d-55805cb3721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import logging\n",
    "from typing import cast, Dict, List, Tuple, Union\n",
    "from typing_extensions import get_args, Literal\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append('/users/sanand14/data/sanand14/learning_dynamics/src/experiments/utils')\n",
    "sys.path.append('/users/sanand14/data/sanand14/learning_dynamics/src/experiments')\n",
    "\n",
    "from aheads import create_repeats_dataset\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import is_square\n",
    "from transformer_lens.head_detector import (compute_head_attention_similarity_score, \n",
    "                      get_previous_token_head_detection_pattern, \n",
    "                      get_duplicate_token_head_detection_pattern,\n",
    "                      get_induction_head_detection_pattern)\n",
    "\n",
    "\n",
    "PYTHIA_VOCAB_SIZE = 50277 #50304\n",
    "N_LAYERS=12\n",
    "MODEL = \"EleutherAI/pythia-160m\"\n",
    "PYTHIA_CHECKPOINTS_OLD = [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512] + list(range(1000, 143000 + 1, 10000)) + [143000]\n",
    "PYTHIA_CHECKPOINTS = [512] + list(range(1000, 10000 + 1, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cade8b7-74df-4c5e-ada9-469b4f5c8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HeadName = Literal[\"previous_token_head\", \"duplicate_token_head\", \"induction_head\"]\n",
    "HEAD_NAMES = cast(List[HeadName], get_args(HeadName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a3f3d7-1203-4760-b3b8-594a801479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repeats_dataset(num_samples=50, min_vector_size=5, max_vector_size=50, min_num_repeats=5, max_num_repeats=20, max_vocab=PYTHIA_VOCAB_SIZE):\n",
    "  \"\"\"Creates a dataset for the experiment.\"\"\"\n",
    "  dataset = []\n",
    "  for _ in range(num_samples):\n",
    "    vector_size = torch.randint(min_vector_size, max_vector_size, (1,)).item()\n",
    "    num_repeats = torch.randint(min_num_repeats, max_num_repeats, (1,)).item()\n",
    "    tokens = torch.randint(0, max_vocab, (1, vector_size))\n",
    "    tokens = tokens.repeat((1, num_repeats))\n",
    "    dataset.append(tokens)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed88b97-5755-4cb5-9252-bcd1f95c5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load('../outputs/aheads/dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa1061d-1a82-4180-975d-304e701a3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b8aaa62-84a9-4c7f-8611-a966b7c29a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attention_head(model1, model2, layer_idx, head_idx, dataset):\n",
    "  if model1.isinstance(HookedTransformer) and model2.isinstance(HookedTransformer):\n",
    "    model1.W_K.data[layer_idx, head_idx, :, :] = model2.W_K.data[layer_idx, head_idx, :, :]\n",
    "    model1.W_Q.data[layer_idx, head_idx, :, :] = model2.W_Q.data[layer_idx, head_idx, :, :]\n",
    "    model1.W_V.data[layer_idx, head_idx, :, :] = model2.W_V.data[layer_idx, head_idx, :, :]\n",
    "    model1.b_K.data[layer_idx, head_idx, :] = model2.b_K.data[layer_idx, head_idx, :]\n",
    "    model1.b_Q.data[layer_idx, head_idx, :] = model2.b_Q.data[layer_idx, head_idx, :]\n",
    "    model1.b_V.data[layer_idx, head_idx, :] = model2.b_V.data[layer_idx, head_idx, :]\n",
    "  else:\n",
    "    model1.encoder.layers[layer_idx].self_attn.in_proj_weight.data[head_idx,:,:] = model2.encoder.layers[layer_idx].self_attn.in_proj_weight.data[head_idx,:,:]\n",
    "  return perplexity(model1, dataset), perplexity(model2, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a891be09-fbcc-4487-aa72-42fdcfb25203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(corpus, model, device=\"cpu\"):\n",
    "    encoded_input = model.to_tokens(corpus)\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model(encoded_input).squeeze(0)\n",
    "      loss = F.cross_entropy(outputs, encoded_input.squeeze(0), reduction='sum')/encoded_input.shape[1]\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe7c40a-eb0a-45a2-a8f6-86b2a581175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, dataset):\n",
    "  data_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      inputs, targets = batch\n",
    "      outputs = model(inputs)\n",
    "      loss = F.cross_entropy(outputs, targets, reduction='sum')\n",
    "      total_loss += loss.item()\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    return torch.exp(torch.tensor(average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b33722-f007-48c9-bbfd-0a231387a349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
